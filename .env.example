# Copy to .env and adjust. Do not commit .env.
# CNCR_OFFLINE=1 disables cloud (OpenAI) only; local Ollama remains allowed.

# Optional: local LLM explanations (Ollama). Scoring is deterministic; LLM only for explanations.
# LLM_MODE=local
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=jamba2-3b-q6k
# LLM_TIMEOUT_SECONDS=180

# Optional: only if using remote/hybrid LLM or OpenAI embeddings
# EMBEDDING_MODE=openai
# OPENAI_API_KEY=sk-...
